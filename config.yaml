# Natron Transformer Configuration

# Data Configuration
data:
  input_file: "data_export.csv"
  sequence_length: 96
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  
# Feature Engineering
features:
  num_features: 100
  normalize: true
  fill_method: "ffill"

# Label Generation V2
labeling:
  neutral_buffer: 0.001
  lookforward: 3
  volume_threshold: 1.5
  balance_threshold: 0.05
  adaptive_balancing: true

# Model Architecture
model:
  d_model: 256
  nhead: 8
  num_encoder_layers: 6
  dim_feedforward: 1024
  dropout: 0.1
  max_seq_length: 96

# Training - Phase 1 (Pretraining)
pretrain:
  enabled: true
  epochs: 50
  batch_size: 128
  learning_rate: 0.0001
  weight_decay: 0.00001
  mask_ratio: 0.15
  temperature: 0.07
  checkpoint_dir: "model/pretrain"

# Training - Phase 2 (Supervised)
supervised:
  epochs: 100
  batch_size: 64
  learning_rate: 0.0001
  weight_decay: 0.00001
  gradient_clip: 1.0
  early_stopping_patience: 15
  checkpoint_dir: "model/supervised"
  
  # Loss weights for multi-task
  loss_weights:
    buy: 1.0
    sell: 1.0
    direction: 1.5
    regime: 1.2

# Training - Phase 3 (RL)
rl:
  enabled: false
  algorithm: "ppo"
  total_timesteps: 100000
  learning_rate: 0.0003
  reward_profit_weight: 1.0
  reward_turnover_penalty: 0.1
  reward_drawdown_penalty: 0.2
  checkpoint_dir: "model/rl"

# Inference API
api:
  host: "0.0.0.0"
  port: 5000
  model_path: "model/natron_v2.pt"
  device: "cuda"

# System
system:
  device: "cuda"
  num_workers: 4
  seed: 42
  log_dir: "logs"
