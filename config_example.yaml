# Natron Transformer Configuration Example
# Copy this file and customize for your setup

data:
  csv_path: "data_export.csv"
  sequence_length: 96
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  neutral_buffer: 0.001
  lookforward: 3
  n_features: 100
  normalize: true
  batch_size: 128
  num_workers: 4
  pin_memory: true

model:
  input_dim: 100
  d_model: 256
  nhead: 8
  num_encoder_layers: 6
  dim_feedforward: 1024
  dropout: 0.1
  activation: "gelu"
  direction_classes: 3
  regime_classes: 6
  use_positional_encoding: true
  max_seq_length: 96

pretrain:
  enabled: true
  epochs: 50
  mask_ratio: 0.15
  learning_rate: 0.0001
  weight_decay: 0.00001
  warmup_epochs: 5
  temperature: 0.07
  contrastive_weight: 0.3
  reconstruction_weight: 0.7
  save_every: 10
  checkpoint_dir: "checkpoints/pretrain"

train:
  epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.00001
  gradient_clip: 1.0
  buy_weight: 1.0
  sell_weight: 1.0
  direction_weight: 1.5
  regime_weight: 1.2
  use_class_weights: true
  focal_loss: true
  focal_alpha: 0.25
  focal_gamma: 2.0
  scheduler: "reduce_on_plateau"
  scheduler_patience: 5
  scheduler_factor: 0.5
  early_stopping_patience: 15
  save_best: true
  checkpoint_dir: "checkpoints/supervised"

rl:
  enabled: false
  algorithm: "ppo"
  episodes: 1000
  steps_per_episode: 500
  profit_reward: 1.0
  turnover_penalty: 0.01
  drawdown_penalty: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5
  learning_rate: 0.0003
  checkpoint_dir: "checkpoints/rl"

inference:
  model_path: "model/natron_v2.pt"
  device: "cuda"
  batch_size: 1
  confidence_threshold: 0.6
  api_host: "0.0.0.0"
  api_port: 5000
  api_debug: false
  socket_host: "0.0.0.0"
  socket_port: 8765
  max_latency_ms: 50

# Global settings
seed: 42
device: "cuda"
mixed_precision: true
log_dir: "logs"
log_level: "INFO"
wandb_enabled: false
wandb_project: "natron-transformer"
output_dir: "output"
model_dir: "model"
